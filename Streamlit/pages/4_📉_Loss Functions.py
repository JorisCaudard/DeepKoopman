import streamlit as st

st.set_page_config(
    page_title="Loss functions",
    page_icon="üìâ",
    layout= 'wide'
)

st.title("Loss functions")

tab1, tab2 = st.tabs(["Fonctions de perte th√©oriques", "Fonction de perte impl√©ment√©e"])

with tab1:
    st.header("Optimisation Th√©orique", divider= True)
        
    st.markdown(r'''Dans le cadre th√©orique de l'article, les auteurs proposent l'optimisation d'une fonction de perte complexe, combinaison lin√©aire de diff√©rentes fonctions.''')
    
    st.subheader("Perte de reconnaissance")

    with st.container(border= True):
        st.latex(r"{\cal L}_{\mathrm{recon}} = \left\| {{\mathbf{x}}_1 - \varphi ^{ - 1}(\varphi ({\mathbf{x}}_1))} \right\|_{\mathrm{MSE}}")

    st.markdown("Cette fonction de perte permet d'assurer la reconnaissance de l'√©tat initial apr√®s passage dans l'auto-encoder (sans op√©rateur de Koopman).")
    
    st.subheader("Perte de Pr√©diction")

    with st.container(border= True):
        st.latex(r"{\cal L}_{\mathrm{pred}} = \frac{1}{{S_p}}\sum_{m = 1}^{S_p} \left\| {{\mathbf{x}}_{m + 1} - \varphi ^{ - 1}(K^m\varphi ({\mathbf{x}}_1))} \right\|_{\mathrm{MSE}}")

    st.markdown(r"Cette fonction de perte permet de calculer la moyenne des erreurs MSE de pr√©dictions sur une sous-trajectoires, d√©not√©es par le param√®tre $S_p$.")

    st.subheader('Perte de Lin√©arit√©')

    with st.container(border= True):
        st.latex(r"{\cal L}_{\mathrm{lin}} = \frac{1}{{T - 1}}\sum_{m = 1}^{T - 1} \left\| {\varphi ({\mathbf{x}}_{m + 1}) - K^m\varphi ({\mathbf{x}}_1)} \right\|_{\mathrm{MSE}}")

    st.markdown(r"Cette fonction de perte permet d'assurer la lin√©arit√© de l'op√©rateur de Koopman sur l'enesmble de la trajectoire.")


    st.subheader("Perte en norme infinie")

    with st.container(border= True):
        st.latex(r"{\cal L}_\infty = \left\| {{\mathbf{x}}_1 - \varphi ^{ - 1}(\varphi ({\mathbf{x}}_1))} \right\|_\infty + \left\| {{\mathbf{x}}_2 - \varphi ^{ - 1}(K\varphi ({\mathbf{x}}_1))} \right\|_\infty")

    st.markdown(r"Les donn√©es d'entra√Ænement ayant √©t√©s g√©n√©r√©es par r√©solution d'√©quation diff√©rentielle, elles ne sont pas bruit√©es ; cette fonction de perte permet de p√©naliser le plus grand √©cart de trajectoire apr√®s une pr√©diction.")

    st.subheader("P√©nalisation sur les poids")

    with st.container(border= True):
        st.latex(r"\left\| {\mathbf{W}} \right\|_2^2")

    st.markdown("Enfin, les op√©rateurs proposent d'ajouter une p√©nalisation sur l'ensemble des poids du mod√®les, afin d'√©viter l'overfitting.")


    
with tab2:
    st.header("Impl√©mentation en Python")

    st.markdown(r"""Dans le cadre de notre impl√©mentation, cette fonction de perte g√©n√©rale a √©t√© simplifi√©e. En effet, la matrice de l'op√©rateur √©tant d√©finie par une couche lin√©aire (sans r√©seau auxiliaire), la lin√©arit√© est assur√©e par la structure m√™me du r√©seau.
                De plus, la perte de reconnaissance est assimil√©e ici √† la perte de pr√©diction (pour m=0). Enfin, les fonctions de pertes infinies et de p√©nalisation ont √©t√© √©cart√©es dans le cadre d'une premi√®re impl√©mentation.
                Nous nous sommes donc focalis√© sur la fonction de perte de pr√©diction dans le cadre de cette impl√©mentation : une somme de fonctions de pertes MSE sur chaque √©tape de la pr√©diction.""")
    
    st.code("""class LossFunction(nn.Module):
    \"""Classe contenant la fonction de perte du r√©seau de Koopman.

    Args:
        numShifts (int): Longueur de la trajectoire sur laquelle la fonction de perte est √©valu√©e.
    \"""
    def __init__(self, params):
        \"""Initialise la fonction de perte avec les param√®tres donn√©s.

        Args:
            params (dict): Un dictionnaire contenant la configuration initiale de la fonction de perte.
                Ce dictionnaire doit inclure le param√®tre numShifts.
        \"""
        super().__init__()

        self.numShifts = params['numShifts']

    def forward(self, targetTrajectory, predictedTrajectory):
        \"""Calcule la fonction de perte entre deux trajectoires.

        Args:
            targetTrajectory (list): Trajectoire r√©elle √† approcher.
            predictedTrajectory (list): Trajdectoire pr√©dite par un r√©seau de neurones de Koopman*

        Returns:
            lossPred (torch.Tensor): Perte calcul√©e entre les deux trajectoires. Le torch.Tensor contient la valeur de la fonction de perte et le gradient associ√©. 
        \"""

        #We compute the Prediction loss
        lossPred = 0

        for m in range(self.numShifts):
            lossPred += F.mse_loss(targetTrajectory[m], predictedTrajectory[m])

        return lossPred""")